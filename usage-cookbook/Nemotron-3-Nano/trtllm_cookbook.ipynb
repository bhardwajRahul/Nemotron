{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying NVIDIA Nemotron-3-Nano with TensorRT LLM\n",
    "\n",
    "This notebook will walk you through how to run the `nnvidia/NVIDIA-Nemotron-3-Nano-30B-A3B` model via TensorRT-LLM\n",
    "\n",
    "[TensorRT LLM](https://nvidia.github.io/TensorRT-LLM/) is NVIDIAâ€™s open-source library for accelerating and optimizing LLM inference performance on NVIDIA GPUs. TRTLLM support for this model is enabled through the AutoDeploy workflow. More details about this workflow can be found [here](https://nvidia.github.io/TensorRT-LLM/features/auto_deploy/auto-deploy.html).\n",
    "\n",
    "For more details on the model [click here](https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8). \n",
    "\n",
    "Prerequisites:\n",
    "- NVIDIA GPU with recent drivers (â‰¥ 60 GB VRAM for BF16, â‰¥ 32 GB for FP8) and CUDA 12.x\n",
    "- Python 3.10+\n",
    "- TensorRT-LLM (you can refer to NVIDIA documentation, or pull this [container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tensorrt-llm/containers/release?version=1.2.0rc5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch on NVIDIA Brev\n",
    "You can simplify the environment setup by using [NVIDIA Brev](https://developer.nvidia.com/brev). Click the button to launch this project on a Brev instance with the necessary dependencies pre-configured.\n",
    "\n",
    "Once deployed, click on the \"Open Notebook\" button to get started with this guide. \n",
    "\n",
    "[![Launch on Brev](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-36ikYKeRmXqG8MJjxsgROJM4S2V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites & environment\n",
    "\n",
    "Set up a containerized environment for TensorRT-LLM by running the following command in a terminal.\n",
    "\n",
    "```shell\n",
    "docker run --rm -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --gpus=all -p 8000:8000 nvcr.io/nvidia/tensorrt-llm/release:1.2.0rc5\n",
    "```\n",
    "\n",
    "You now have TRT-LLM set up! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /tmp/tmpuvjbo8y1\n",
      "Requirement already satisfied: pip in ./.venv/lib/python3.12/site-packages (25.0.1)\n"
     ]
    }
   ],
   "source": [
    "#If pip not found\n",
    "!python -m ensurepip --default-pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install torch openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify GPU\n",
    "\n",
    "Check that CUDA is available and the GPU is detected correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.12 (main, Dec  9 2025, 19:02:36) [Clang 21.1.4 ]\n",
      "CUDA available: True\n",
      "Num GPUs: 1\n",
      "GPU[0]: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "# Environment check\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Num GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU[{i}]: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI-compatible server\n",
    "\n",
    "Start a local OpenAI-compatible server with TensorRT-LLM via the terminal, within the running docker container.\n",
    "\n",
    "Ensure that the following commands are executed from the docker terminal.\n",
    "\n",
    "#### Create a YAML file with the required configuration\n",
    "\n",
    "```shell\n",
    "cat > nano_v3.yaml<<EOF\n",
    "runtime: trtllm\n",
    "compile_backend: torch-cudagraph\n",
    "max_batch_size: 64\n",
    "max_seq_len: 16384\n",
    "enable_chunked_prefill: true\n",
    "attn_backend: flashinfer\n",
    "model_factory: AutoModelForCausalLM\n",
    "skip_loading_weights: false\n",
    "free_mem_ratio: 0.65\n",
    "cuda_graph_batch_sizes: [1, 2, 4, 8, 16, 24, 32, 64, 128, 256, 320, 384]\n",
    "kv_cache_config:\n",
    "  # disable kv_cache reuse since not supported for hybrid/ssm models\n",
    "  enable_block_reuse: false\n",
    "transforms:\n",
    "  detect_sharding:\n",
    "    sharding_dims: ['ep', 'bmm']\n",
    "    allreduce_strategy: 'AUTO'\n",
    "    manual_config:\n",
    "      head_dim: 128\n",
    "      tp_plan:\n",
    "        # mamba SSM layer\n",
    "        \"in_proj\": \"mamba\"\n",
    "        \"out_proj\": \"rowwise\"\n",
    "        # attention layer\n",
    "        \"q_proj\": \"colwise\"\n",
    "        \"k_proj\": \"colwise\"\n",
    "        \"v_proj\": \"colwise\"\n",
    "        \"o_proj\": \"rowwise\"\n",
    "        # NOTE: consider not sharding shared experts and/or\n",
    "        # latent projections at all, keeping them replicated.\n",
    "        # To do so, comment out the corresponding entries.\n",
    "        # moe layer: SHARED experts\n",
    "        \"up_proj\": \"colwise\"\n",
    "        \"down_proj\": \"rowwise\"\n",
    "        # MoLE: latent projections: simple shard\n",
    "        \"fc1_latent_proj\": \"gather\"\n",
    "        \"fc2_latent_proj\": \"gather\"\n",
    "  multi_stream_moe:\n",
    "    stage: compile\n",
    "    enabled: true\n",
    "  insert_cached_ssm_attention:\n",
    "      cache_config:\n",
    "        mamba_dtype: float32\n",
    "  fuse_mamba_a_log:\n",
    "    stage: post_load_fusion\n",
    "    enabled: true\n",
    "EOF\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model\n",
    "\n",
    "##### BF16 version\n",
    "\n",
    "```shell\n",
    "TRTLLM_ENABLE_PDL=1 trtllm-serve \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\" \\\n",
    "--host 0.0.0.0 \\\n",
    "--port 8000 \\\n",
    "--backend _autodeploy \\\n",
    "--trust_remote_code \\\n",
    "--reasoning_parser deepseek-r1 \\\n",
    "--tool_parser qwen3_coder \\\n",
    "--extra_llm_api_options nano_v3.yaml\n",
    "```\n",
    "\n",
    "##### Alternative: Load the FP8 quantized version for faster inference and lower memory usage\n",
    "\n",
    "```shell\n",
    "TRTLLM_ENABLE_PDL=1 trtllm-serve \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8\" \\\n",
    "--host 0.0.0.0 \\\n",
    "--port 8000 \\\n",
    "--backend _autodeploy \\\n",
    "--trust_remote_code \\\n",
    "--reasoning_parser deepseek-r1 \\\n",
    "--tool_parser qwen3_coder \\\n",
    "--extra_llm_api_options nano_v3.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your server is now running!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the API\n",
    "\n",
    "Use the OpenAI-compatible client to send requests to the TensorRT-LLM server.\n",
    "\n",
    "Note: The model supports two modes - Reasoning ON (default) vs OFF. This can be toggled by setting enable_thinking to False, as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "\n",
    "# Setup client\n",
    "BASE_URL = \"http://0.0.0.0:8000/v1\"\n",
    "API_KEY = \"null\" \n",
    "client = OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
    "\n",
    "model_id = \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\" # set this to the model you loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning on\n",
      "We need to respond with 3 bullet points about TensorRT-LLM. Provide concise, factual points. Should we add any extra explanation? They just asked for 3 bullet points. We'll produce exactly 3 bullet points.\n",
      " \n",
      "- **Highâ€‘performance inference**: TensorRTâ€‘LLM leverages NVIDIAâ€™s TensorRT optimizations (kernel fusion, INT8/FP8 quantization, and graph-level transformations) to accelerate largeâ€‘languageâ€‘model generation on GPUs, delivering up toâ€¯10Ã— lower latency and higher throughput compared to naÃ¯ve implementations.  \n",
      "\n",
      "ourgwardâ€™s LLM inference engine built on NVIDIAâ€™s **TensorRT-LLM** library efficiently maps transformer operators to Tensor Core kernels, enabling mixedâ€‘precision (BF16/FP8) execution while preserving model accuracy.  \n",
      "\n",
      "- **Scalable across models and hardware**: Supports a wide range of architectures (GPTâ€‘J, LLaMA, Falcon, etc.) and GPU families (A100, H100, RT participou) with dynamic tensor shapes and automatic batching, allowing deployment from edge devices to massive multiâ€‘GPU clusters without retraining.  \n",
      "\n",
      "- **Developerâ€‘friendly tooling**: Provides Python and C++ APIs\n",
      "\n",
      "\n",
      "Reasoning off\n",
      "- **Highâ€‘performance inference engine**: TensorRTâ€‘LLM is NVIDIAâ€™s optimized library for deploying large language models on GPUs, delivering up toâ€¯2â€‘3Ã— higher throughput and lower latency than vanilla PyTorch/TF inference through kernel fusion, kernel autoâ€‘tuning, and INT8/FP8 quantization.  \n",
      "- **Seamless integration with existing LLMs**: It provides a dropâ€‘in Python API and supports popular model formats (e.g., Huggingâ€¯Face ðŸ¤— Transformers, ONNX, and TensorFlow SavedModel), enabling developers to convert and run models like Llamaâ€‘2, GPTâ€‘NeoX, and Falcon without rewriting code.  \n",
      "- **Scalable multiâ€‘model serving**: Built on Triton Inference Server, TensorRTâ€‘LLM supports concurrent execution of multiple LLMs, dynamic batching, and tensorâ€‘parallelism across GPUs, making it suitable for productionâ€‘scale serving in cloud, edge, and onâ€‘premise environments. \n"
     ]
    }
   ],
   "source": [
    "# Reasoning on (default)\n",
    "print(\"Reasoning on\")\n",
    "response = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me 3 bullet points about TensorRT-LLM.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=256,\n",
    ")\n",
    "print(response.choices[0].message.reasoning_content, response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Reasoning off\n",
    "print(\"Reasoning off\")\n",
    "response = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me 3 bullet points about TensorRT-LLM.\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=256,\n",
    "    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}\n",
    ")\n",
    "print(response.choices[0].message.reasoning_content, response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "\n",
      "The first 5 prime numbers are:  \n",
      "**2, 3, 5, 7, 11**\n",
      "\n",
      "### Why?\n",
      "- **Prime numbers**"
     ]
    }
   ],
   "source": [
    "# Streaming chat completion\n",
    "print(\"Streaming response:\")\n",
    "stream = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the first 5 prime numbers?\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool calling\n",
    "\n",
    "Use the OpenAI tools schema to call functions via the TensorRT-LLM endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user wants to calculate a 15% tip on a $50 bill. Let me check the tools available. There's a function called calculate_tip that takes bill_total and tip_percentage. The parameters are integers, so I need to convert 15% to 15 and $50 to 50. I'll call the function with these values. The function should return the tip amount, which I can then present to the user.\n",
      " \n",
      "\n",
      "\n",
      "[ChatCompletionMessageFunctionToolCall(id='chatcmpl-tool-a516606befc240c893817b3480c0db5b', function=Function(arguments='{\"bill_total\": 50, \"tip_percentage\": 15}', name='calculate_tip'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "# Tool calling via OpenAI tools schema\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_tip\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"bill_total\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The total amount of the bill\"\n",
    "                    },\n",
    "                    \"tip_percentage\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The percentage of tip to be applied\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"bill_total\", \"tip_percentage\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": \"My bill is $50. What will be the amount for 15% tip?\"}\n",
    "    ],\n",
    "    tools=TOOLS,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    max_tokens=512,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.reasoning_content, completion.choices[0].message.content)\n",
    "print(completion.choices[0].message.tool_calls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
