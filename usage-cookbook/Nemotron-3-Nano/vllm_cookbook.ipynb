{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying NVIDIA Nemotron-3-Nano with vLLM\n",
    "\n",
    "This notebook will walk you through how to run the `nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B` model with vLLM.\n",
    "\n",
    "[vLLM](https://docs.vllm.ai) is a fast and easy-to-use library for LLM inference and serving. \n",
    "\n",
    "For more details on the model [click here](https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8)\n",
    "\n",
    "Prerequisites:\n",
    "- NVIDIA GPU with recent drivers (‚â• 60 GB VRAM for BF16, ‚â• 32 GB for FP8) and CUDA 12.x\n",
    "- Python 3.10+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch on NVIDIA Brev\n",
    "You can simplify the environment setup by using [NVIDIA Brev](https://developer.nvidia.com/brev). Click the button to launch this project on a Brev instance with the necessary dependencies pre-configured.\n",
    "\n",
    "Once deployed, click on the \"Open Notebook\" button to get started with this guide. \n",
    "\n",
    "[![Launch on Brev](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-36ikINrMffBCbrtTVLr6MFcllcs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /tmp/tmpj3nyk2jv\n",
      "Processing /tmp/tmpj3nyk2jv/pip-25.0.1-py3-none-any.whl\n",
      "Installing collected packages: pip\n",
      "Successfully installed pip-25.0.1\n"
     ]
    }
   ],
   "source": [
    "#If pip not found\n",
    "!python -m ensurepip --default-pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install vllm torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify GPU\n",
    "\n",
    "Confirm CUDA is available and your GPU is visible to PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Num GPUs: 1\n",
      "GPU[0]: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "# GPU environment check\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Num GPUs: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU[{i}]: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model\n",
    "\n",
    "Initialize the Nemotron model in vLLM with BF16 for efficient GPU inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shadeform/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-12 19:00:05 [utils.py:253] non-default args: {'trust_remote_code': True, 'seed': None, 'disable_log_stats': True, 'model': 'nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-12 19:00:05 [arg_utils.py:1175] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 19:00:07,029\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-12 19:00:07 [model.py:637] Resolved architecture: NemotronHForCausalLM\n",
      "INFO 12-12 19:00:07 [model.py:1750] Using max model len 262144\n",
      "INFO 12-12 19:00:07 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 12-12 19:00:07 [config.py:315] Disabling cascade attention since it is not supported for hybrid models.\n",
      "INFO 12-12 19:00:07 [config.py:439] Setting attention block size to 1072 tokens to ensure that attention page size is >= mamba page size.\n",
      "INFO 12-12 19:00:07 [config.py:463] Padding mamba page size by 1.13% to ensure that mamba page size and attention page size are exactly equal.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:08 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16', speculative_config=None, tokenizer='nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=262144, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:09 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.122.108:51693 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:09 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:09 [gpu_model_runner.py:3467] Starting to load model nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:09 [layer.py:379] Enabled separate cuda stream for MoE shared_experts\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:10 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/13 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   8% Completed | 1/13 [00:00<00:11,  1.03it/s]\n",
      "Loading safetensors checkpoint shards:  15% Completed | 2/13 [00:01<00:08,  1.34it/s]\n",
      "Loading safetensors checkpoint shards:  23% Completed | 3/13 [00:02<00:08,  1.20it/s]\n",
      "Loading safetensors checkpoint shards:  31% Completed | 4/13 [00:03<00:07,  1.18it/s]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 5/13 [00:04<00:07,  1.11it/s]\n",
      "Loading safetensors checkpoint shards:  46% Completed | 6/13 [00:05<00:06,  1.11it/s]\n",
      "Loading safetensors checkpoint shards:  54% Completed | 7/13 [00:06<00:05,  1.14it/s]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 8/13 [00:07<00:04,  1.12it/s]\n",
      "Loading safetensors checkpoint shards:  69% Completed | 9/13 [00:07<00:03,  1.10it/s]\n",
      "Loading safetensors checkpoint shards:  77% Completed | 10/13 [00:08<00:02,  1.13it/s]\n",
      "Loading safetensors checkpoint shards:  85% Completed | 11/13 [00:09<00:01,  1.10it/s]\n",
      "Loading safetensors checkpoint shards:  92% Completed | 12/13 [00:10<00:00,  1.09it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 13/13 [00:11<00:00,  1.13it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 13/13 [00:11<00:00,  1.13it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:22 [default_loader.py:308] Loading weights took 11.60 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:22 [gpu_model_runner.py:3549] Model loading took 58.9076 GiB memory and 12.339329 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:25 [backends.py:655] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/b9f8ab6b7d/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:25 [backends.py:715] Dynamo bytecode transform time: 2.73 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:26 [backends.py:257] Cache the graph for dynamic shape for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:27 [backends.py:288] Compiling a graph for dynamic shape takes 1.69 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:29 [fused_moe.py:875] Using configuration from /home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=1856,device_name=NVIDIA_H100_80GB_HBM3.json for MoE layer.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:31 [monitor.py:34] torch.compile takes 4.43 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:32 [gpu_worker.py:359] Available KV cache memory: 7.37 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m WARNING 12-12 19:00:32 [kv_cache_utils.py:1028] Add 1 padding layers, may waste at most 4.35% KV cache memory\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:32 [kv_cache_utils.py:1286] GPU KV cache size: 257,280 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:32 [kv_cache_utils.py:1291] Maximum concurrency for 262,144 tokens per request: 4.82x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:13<00:00,  3.70it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:05<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:52 [gpu_model_runner.py:4466] Graph capturing finished in 20 secs, took 1.39 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:52 [core.py:254] init engine (profile, create kv cache, warmup model) took 29.86 seconds\n",
      "INFO 12-12 19:00:54 [llm.py:343] Supported tasks: ['generate']\n",
      "Model ready\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\",\n",
    "    # Alternative: Load the FP8 quantized version for faster inference and lower memory usage\n",
    "    # model=\"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Model ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate responses\n",
    "\n",
    "Generate text with vLLM using single, batched, and simple streaming examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single or batch prompts\n",
    "\n",
    "Send one prompt or a list to run batched generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 366.73it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:23<00:00, 23.45s/it, est. speed input: 0.47 toks/s, output: 8.53 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answer in accordance with the format: your answer must contain exactly 3 bullet points. Use the markdown bullet points such as:\n",
      "* This is point 1. \n",
      "* This is point 2\n",
      "\n",
      "answer:\"\n",
      "\n",
      "We need to output exactly 3 bullet points, using markdown bullet points \"*\". So we need to give three bullet points about vLLM. Should be concise. Ensure exactly 3 bullet points, no extra text. No extra lines before or after? Probably just three bullet points. Ensure no extra bullet points or extra text. Provide exactly three lines each starting with \"* \". No extra blank lines. Let's produce:\n",
      "\n",
      "* vLLM is an open-source library for efficient large language model inference.\n",
      "* It supports high-throughput and low-latency serving via PagedAttention.\n",
      "* It enables easy scaling and deployment of LLMs across multiple GPUs and platforms.\n",
      "\n",
      "That's three bullet points. Ensure no extra text.\n",
      "</think>\n",
      "* vLLM is an open‚Äësource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 2030.48it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.60it/s, est. speed input: 9.60 toks/s, output: 235.75 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1: 'Hello, my name is'\n",
      " NAME_1. Let's chat!\" The placeholder NAME_1 likely should be replaced with something? The user gave that as instruction; they might want the assistant to adopt that name? The instruction says \"You are a chat bot, your goal is to continue the conversation between Bot and Visitor.\" The example shows Bot says \"Hello, my name is NAME_1. Let's chat!\" So we should continue from that. The bot introduced itself as NAME_1. So we need to respond as Visitor? Or as Bot? The user says \"continue the conversation between Bot and Visitor.\" So we need to produce the next turn. The Bot already said greeting. So Visitor should reply. Probably we should respond as Visitor with a greeting and maybe ask how Bot is. Then Bot replies, etc. But the instruction: \"You are a chat bot, your goal is to continue the conversation between Bot and Visitor.\" So we need to output the next messages? Probably we\n",
      "\n",
      "Prompt 2: 'The capital of France is'\n",
      " Paris.\" with no extra words or explanation. This is a straightforward request, not disallowed. We can comply. Provide exactly that sentence, no extra words.\n",
      "</think>\n",
      "The capital of France is Paris.\n",
      "\n",
      "Prompt 3: 'Explain quantum computing in simple terms:'\n",
      " like a library with many books that can be in many states at once, etc. Provide analogies. Should be friendly. No disallowed content. Provide answer.\n",
      "</think>\n",
      "**Quantum Computing in a Nutshell (with a cozy knitting analogy)**  \n",
      "\n",
      "---\n",
      "\n",
      "### 1. The Classical Computer: A Simple Stitch\n",
      "- **Bit = a single stitch** ‚Äì it can be either **0** (off) or **1** (on).  \n",
      "- By chaining many stitches together, a classical computer can represent any pattern, but each stitch has only one color at a time.\n",
      "\n",
      "### 2. The Quantum Computer: A Magical Yarn\n",
      "- **Qubit = a ‚Äúquantum stitch‚Äù** ‚Äì it can be **0, 1, or a blend of both** at the same time.  \n",
      "- In knitting terms, imagine a yarn that can be **half red, half blue, and also shimmering** all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "params = SamplingParams(temperature=0.6, max_tokens=200)\n",
    "\n",
    "# Single prompt\n",
    "single = llm.generate([\"Give me 3 bullet points about vLLM.\"], sampling_params=params)\n",
    "print(single[0].outputs[0].text)\n",
    "\n",
    "# Batch prompts\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The capital of France is\",\n",
    "    \"Explain quantum computing in simple terms:\"\n",
    "]\n",
    "outputs = llm.generate(prompts, sampling_params=params)\n",
    "for i, out in enumerate(outputs):\n",
    "    print(f\"\\nPrompt {i+1}: {out.prompt!r}\")\n",
    "    print(out.outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Streamed generation\n",
    "\n",
    "Print characters as they are produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1157.37it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.49s/it, est. speed input: 3.21 toks/s, output: 205.41 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  Also mention \"Raiden Shogun\". Ensure haiku format: 5-7-5 syllables.\n",
      "\n",
      "We can produce:\n",
      "\n",
      "\"Silicon thunder,\n",
      "Raiden's will in silicon,\n",
      "Electrons pulse, swift.\"\n",
      "\n",
      "But that's not correct syllable count. Let's craft:\n",
      "\n",
      "\"Lightning cores ignite (5)\n",
      "Raiden's will on silicon (7)\n",
      "Sparks of fate arise (5)\n",
      "\n",
      "But need 5-7-5. Let's count.\n",
      "\n",
      "Line1: \"Lightning cores ignite\" -> Light-ning (2) cores (1) i-gnite (2) = 5? Let's count: Light(1) ning(1) = 2? Actually \"lightning\" is 2 syllables. \"cores\" 1, \"ignite\" 2 => total 5. Good.\n",
      "\n",
      "Line2: \"Raiden's will on silicon\" -> count: Ra-i-den's (3? Actually \"Raiden\" is 2 syllables? It's \"Ry-deen\"? Usually 2? In English \"Raiden\" is 2 syllables: \"Ry-den\". With possessive \"Raiden's\" still 2. \"will\" 1, \"on\" 1, \"silicon\" 3? \"si-li-con\" 3. So total 2+1+1+3 = 7. Good.\n",
      "\n",
      "Line3: \"Sparks of fate arise\" -> Sparks (1) of (1) fate (1) a-rise (2) = 5. Good.\n",
      "\n",
      "Thus haiku:\n",
      "\n",
      "Lightning cores ig"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nite  \n",
      "Raiden's will on silicon  \n",
      "Sparks of fate arise\n",
      "\n",
      "We can also mention \"GPU\" explicitly. Maybe \"GPU\" in line2? But we already have silicon. Could incorporate \"GPU\" but keep syllable count.\n",
      "\n",
      "Maybe:\n",
      "\n",
      "\"Silicon thunder (5?) Count: Si-li-con (3) thun-der (2) = 5. Good.\n",
      "\n",
      "\"Raiden's will in GPU\" Count: Ra-i-den's (2) will (1) in (1) G-P-U (3?) Actually \"GPU\" pronounced \"gee-pee-you\" 3 syllables. So total 2+1+1+3 = 7. Good.\n",
      "\n",
      "\"Electrons blaze\" Count: Elec-trons (3) blaze (1) = 4, need 5.\n"
     ]
    }
   ],
   "source": [
    "def stream_like(prompt: str, llm: LLM, sampling_params: SamplingParams) -> None:\n",
    "    outputs = llm.generate([prompt], sampling_params=sampling_params)\n",
    "    text = outputs[0].outputs[0].text\n",
    "    print(\"Response:\", end=\" \")\n",
    "    for ch in text:\n",
    "        print(ch, end=\"\", flush=True)\n",
    "    print()\n",
    "\n",
    "stream_like(\"Write a haiku about GPUs.\", llm, SamplingParams(temperature=0.7, max_tokens=512))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI-compatible server\n",
    "\n",
    "Serve the model via an OpenAI-compatible API using vLLM.\n",
    "\n",
    "Before starting the server:\n",
    "- Restart the kernel to free GPU memory used by the in-process LLM\n",
    "- Ensure you use the same virtual environment with installed dependencies in your terminal. To do this within your Brev instance, open a terminal and run:\n",
    "  ```shell\n",
    "  source /home/shadeform/.venv/bin/activate\n",
    "  ```\n",
    "- Choose the desired model (FP8 or BF16). The snippet below pulls the BF16 version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After restarting the kernel, run this in a terminal:\n",
    "\n",
    "```shell\n",
    "git clone https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\n",
    "```\n",
    "\n",
    "```shell\n",
    "python3 -m vllm.entrypoints.openai.api_server \\\n",
    "    --model \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\" \\\n",
    "    --dtype auto \\\n",
    "    --trust-remote-code \\\n",
    "    --served-model-name nemotron \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 5000 \\\n",
    "    --enable-auto-tool-choice \\\n",
    "    --tool-call-parser qwen3_coder \\\n",
    "    --reasoning-parser-plugin \"NVIDIA-Nemotron-3-Nano-30B-A3B-BF16/nano_v3_reasoning_parser.py\" \\\n",
    "    --reasoning-parser nano_v3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your server is now running!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the API\n",
    "\n",
    "Send chat and streaming requests to your vLLM server using the OpenAI-compatible client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The model supports two modes - Reasoning ON (default) vs OFF. This can be toggled by setting enable_thinking to False, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client: Standard chat and streaming\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:5000/v1\", api_key=\"null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning on\n",
      "Reasoning: We need to output a haiku about GPUs. Haiku is 5-7-5 syllables, about GPUs.\n",
      "\n",
      "We'll produce a haiku.\n",
      "\n",
      "Need ensure correct syllable count.\n",
      "\n",
      "Potential haiku:\n",
      "\n",
      "\"Silicon heart thrums / parallel streams blaze night and day / fire forged in clay.\"\n",
      "\n",
      "Let's count syllables:\n",
      "\n",
      "Silicon-heart thrums = Si-li-con (3) heart (1) thrums (1) =5? Actually \"Silicon\" is 3 syllables (Si-li-con). \"heart\" is 1, \"thrums\" 1 => total 5. Good.\n",
      "\n",
      "parallel streams blaze night and day = par-allel (3) streams (1) blaze (1) night (1) and (1) day (1) =8? Let's count properly: \"parallel\" = 3 syllables (par-al-llel? Actually typically 3: par-al-lel). \"streams\" = 1, \"blaze\" =1, \"night\" =1, \"and\" =1, \"day\" =1 => total 3+1+1+1+1+1 =8 syllables. That's too many. Need 7.\n",
      "\n",
      " \n",
      "Content: None\n",
      "\n",
      "\n",
      "Reasoning off\n",
      "Here are 3 interesting facts about **vLLM** (a high-performance library for serving large language models):\n",
      "\n",
      "1. **PagedAttention: Revolutionizing Memory Management**  \n",
      "   vLLM introduces **PagedAttention**, a novel memory management technique inspired by virtual memory paging in operating systems. Instead of requiring contiguous GPU memory for KV caches (which causes fragmentation and limits batch sizes), it dynamically allocates memory blocks across the GPU. This allows vLLM to serve **2‚Äì4√ó more concurrent requests** with the same hardware (e.g., fitting 24K tokens on a single A100 vs. 6K in Hugging Face Transformers).\n",
      "\n",
      "2. **Native Support for Multi-Turn Conversations**  \n",
      "   Unlike many frameworks that require manual prompt engineering for chat history, vLLM natively handles **multi-turn dialogue** via its `ChatTemplate` system. It automatically structures prompts with system/user/assistant roles, manages context windows, and supports streaming responses‚Äîmaking it ideal for building chatbots without custom preprocessing.\n",
      "\n",
      "3. **Open-Source & Community-Driven Innovation**  \n",
      "   vLLM is **100% open-source** (Apache 2.0 license)\n"
     ]
    }
   ],
   "source": [
    "# Reasoning on (default)\n",
    "print(\"Reasoning on\")\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about GPUs.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=256,\n",
    ")\n",
    "print(\"Reasoning:\", resp.choices[0].message.reasoning_content, \"\\nContent:\", resp.choices[0].message.content)\n",
    "print(\"\\n\")\n",
    "# Reasoning off\n",
    "print(\"Reasoning off\")\n",
    "resp2 = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me 3 interesting facts about vLLM.\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=256,\n",
    "    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}\n",
    ")\n",
    "print(resp2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The first 5 prime numbers are:  \n",
      "**2, 3, 5, 7, 11**.  \n",
      "\n",
      "### Why?\n",
      "- **Prime numbers** are natural numbers greater than 1 that have no positive divisors other than 1 and themselves.\n",
      "- **2** is the smallest prime (and the only even prime).\n",
      "- **3**, **5**, **7**, and **11** follow as the next primes (4, 6, 8, 9, 10 are not prime).\n",
      "\n",
      "### Quick Check:\n",
      "| Number | Divisible by? | Prime? |\n",
      "|--------|---------------|--------|\n",
      "| 2      | 1, 2          | ‚úÖ Yes |\n",
      "| 3      | 1, 3          | ‚úÖ Yes |\n",
      "| 4      | 1, 2, 4       | ‚ùå No  |\n",
      "| 5      | 1, 5          | ‚úÖ Yes |\n",
      "| 6      | 1, 2, 3, 6    | ‚ùå No  |\n",
      "| 7      | 1, 7          | ‚úÖ Yes |\n",
      "| 8, 9, 10 | (not prime)   | ‚ùå No  |\n",
      "| **11** | **1, 11**     | ‚úÖ **Yes** |\n",
      "\n",
      "Thus, the sequence of the first 5 primes is **2, 3, 5, 7, 11**. üåü"
     ]
    }
   ],
   "source": [
    "# Streaming chat completion\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the first 5 prime numbers?\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    delta = chunk.choices[0].delta\n",
    "    if delta and delta.content:\n",
    "        print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool calling\n",
    "\n",
    "Call functions using the OpenAI Tools schema and inspect returned tool_calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user wants to calculate a 15% tip on a $50 bill. Let me check the tools available. There's a calculate_tip function that takes bill_total and tip_percentage. The parameters are required, so I need both. The bill is $50, and the tip percentage is 15. I should call the function with these values. Let me make sure the parameters are integers. Yes, 50 and 15 are both integers. So the tool call should be calculate_tip with bill_total 50 and tip_percentage 15. That should give the tip amount.\n",
      "\n",
      "[ChatCompletionMessageFunctionToolCall(id='chatcmpl-tool-b58e15d0f14b61c3', function=Function(arguments='{\"bill_total\": 50, \"tip_percentage\": 15}', name='calculate_tip'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "# Tool calling via OpenAI tools schema\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_tip\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"bill_total\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The total amount of the bill\"\n",
    "                    },\n",
    "                    \"tip_percentage\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The percentage of tip to be applied\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"bill_total\", \"tip_percentage\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": \"My bill is $50. What will be the amount for 15% tip?\"}\n",
    "    ],\n",
    "    tools=TOOLS,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    max_tokens=512,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.reasoning_content)\n",
    "print(completion.choices[0].message.tool_calls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
