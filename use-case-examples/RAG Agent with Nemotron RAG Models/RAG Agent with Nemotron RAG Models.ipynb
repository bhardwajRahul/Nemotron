{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Agent with Hugging Face NVIDIA Models + NVIDIA AI Endpoints LLM\n",
        "\n",
        "This notebook demonstrates how to build a RAG (Retrieval-Augmented Generation) agent using:\n",
        "- **Hugging Face versions** of NVIDIA's Llama 3.2 models for embeddings and reranking\n",
        "- **NVIDIA AI Endpoints** for the LLM (through build.nvidia.com)\n",
        "\n",
        "## Models Used:\n",
        "- **Embedding Model**: `nvidia/llama-3.2-nv-embedqa-1b-v2` (Hugging Face)\n",
        "- **Reranking Model**: `nvidia/llama-3.2-nv-rerankqa-1b-v2` (Hugging Face)\n",
        "- **LLM Model**: `nvidia/nvidia-nemotron-nano-9b-v2` (NVIDIA AI Endpoints)\n",
        "\n",
        "## Architecture:\n",
        "- Local Hugging Face models for embedding and reranking (better performance, privacy)\n",
        "- NVIDIA AI Endpoints for LLM (managed service, latest models)\n",
        "- Complete LangGraph agent with retrieval tools\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "âœ… NVIDIA API key found!\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoConfig\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors.base import BaseDocumentCompressor\n",
        "from langchain.schema import Document\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "_LOGGER = logging.getLogger(__name__)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Check for NVIDIA API key\n",
        "if \"NVIDIA_API_KEY\" not in os.environ:\n",
        "    print(\"âš ï¸  Warning: NVIDIA_API_KEY not found in environment variables.\")\n",
        "    print(\"Please set your NVIDIA API key to use the LLM:\")\n",
        "    print(\"export NVIDIA_API_KEY='your_api_key_here'\")\n",
        "else:\n",
        "    print(\"âœ… NVIDIA API key found!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Ingestion Configuration\n",
        "DATA_DIR = Path(\"../data/it-knowledge-base\")\n",
        "CHUNK_SIZE = 800\n",
        "CHUNK_OVERLAP = 120\n",
        "\n",
        "# Model Configuration\n",
        "LLM_MODEL = \"nvidia/nvidia-nemotron-nano-9b-v2\"  # NVIDIA AI Endpoints\n",
        "RETRIEVER_EMBEDDING_MODEL = \"nvidia/llama-3.2-nv-embedqa-1b-v2\"  # Hugging Face\n",
        "RETRIEVER_RERANK_MODEL = \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"  # Hugging Face\n",
        "\n",
        "# Model parameters\n",
        "MAX_LENGTH = 8192\n",
        "EMBEDDING_DIM = 2048\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Custom Hugging Face Embedding Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HuggingFaceNVIDIAEmbeddings(Embeddings):\n",
        "    \"\"\"Custom embedding class for NVIDIA Llama 3.2 embedding model from Hugging Face.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str, device: str = \"auto\", trust_remote_code: bool = True):\n",
        "        self.model_name = model_name\n",
        "        self.device = device if device != \"auto\" else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        print(f\"Loading embedding model: {model_name}\")\n",
        "        \n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=trust_remote_code)\n",
        "        \n",
        "        # Load model - the key is to use trust_remote_code=True for custom NVIDIA models\n",
        "        print(\"Loading model with trust_remote_code=True for custom NVIDIA architecture...\")\n",
        "        self.model = AutoModel.from_pretrained(\n",
        "            model_name, \n",
        "            trust_remote_code=trust_remote_code,\n",
        "        )\n",
        "        \n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        \n",
        "        # Add padding token if it doesn't exist\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        \n",
        "        print(f\"âœ… Embedding model loaded on {self.device}\")\n",
        "    \n",
        "    def average_pool(self, last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Average pooling with attention mask.\"\"\"\n",
        "        last_hidden_states_masked = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
        "        embedding = last_hidden_states_masked.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
        "        embedding = F.normalize(embedding, dim=-1)\n",
        "        return embedding\n",
        "    \n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Embed a list of documents.\"\"\"\n",
        "        # Add document prefix as specified in the model documentation\n",
        "        prefixed_texts = [f\"passage: {text}\" for text in texts]\n",
        "        return self._embed_texts(prefixed_texts)\n",
        "    \n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Embed a single query.\"\"\"\n",
        "        # Add query prefix as specified in the model documentation\n",
        "        prefixed_text = f\"query: {text}\"\n",
        "        embeddings = self._embed_texts([prefixed_text])\n",
        "        return embeddings[0]\n",
        "    \n",
        "    def _embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Internal method to embed texts.\"\"\"\n",
        "        embeddings = []\n",
        "        \n",
        "        # Process in batches to handle memory efficiently\n",
        "        batch_size = 8\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            \n",
        "            # Tokenize\n",
        "            batch_inputs = self.tokenizer(\n",
        "                batch_texts,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=MAX_LENGTH,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(self.device)\n",
        "            \n",
        "            # Generate embeddings\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**batch_inputs)\n",
        "                batch_embeddings = self.average_pool(\n",
        "                    outputs.last_hidden_state, \n",
        "                    batch_inputs[\"attention_mask\"]\n",
        "                )\n",
        "            \n",
        "            # Convert to list and add to results\n",
        "            embeddings.extend(batch_embeddings.cpu().numpy().tolist())\n",
        "        \n",
        "        return embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Custom Hugging Face Reranker Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HuggingFaceNVIDIAReranker(BaseDocumentCompressor):\n",
        "    \"\"\"Custom reranker class for NVIDIA Llama 3.2 reranking model from Hugging Face.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str, device: str = \"auto\", trust_remote_code: bool = True, top_k: int = 3, max_length: int = 512):\n",
        "        # Initialize parent class first\n",
        "        super().__init__()\n",
        "        \n",
        "        # Store configuration\n",
        "        self._model_name = model_name\n",
        "        self._device = device if device != \"auto\" else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self._top_k = top_k\n",
        "        self._max_length = max_length\n",
        "        \n",
        "        print(f\"Loading reranking model: {model_name}\")\n",
        "        \n",
        "        # Load tokenizer with proper configuration\n",
        "        self._tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name, \n",
        "            trust_remote_code=trust_remote_code,\n",
        "            padding_side=\"left\"\n",
        "        )\n",
        "        if self._tokenizer.pad_token is None:\n",
        "            self._tokenizer.pad_token = self._tokenizer.eos_token\n",
        "        \n",
        "        # Load model for sequence classification (reranking)\n",
        "        model_kwargs = {\n",
        "            \"trust_remote_code\": trust_remote_code,\n",
        "        }\n",
        "        \n",
        "        from transformers import AutoModelForSequenceClassification\n",
        "        self._model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            **model_kwargs\n",
        "        ).eval()\n",
        "        \n",
        "        if self._model.config.pad_token_id is None:\n",
        "            self._model.config.pad_token_id = self._tokenizer.eos_token_id\n",
        "        \n",
        "        self._model.to(self._device)\n",
        "        \n",
        "        print(f\"âœ… Reranking model loaded on {self._device}\")\n",
        "    \n",
        "    def _prompt_template(self, query: str, passage: str) -> str:\n",
        "        \"\"\"Format query and passage with the proper prompt template.\"\"\"\n",
        "        return f\"question:{query} \\n \\n passage:{passage}\"\n",
        "    \n",
        "    def compress_documents(\n",
        "        self,\n",
        "        documents: List[Document],\n",
        "        query: str,\n",
        "        callbacks: Optional[Any] = None,\n",
        "    ) -> List[Document]:\n",
        "        \"\"\"Compress documents by reranking them based on relevance to the query.\"\"\"\n",
        "        if not documents:\n",
        "            return documents\n",
        "        \n",
        "        # Calculate relevance scores for all documents in batch\n",
        "        scores = self._calculate_relevance_scores(query, [doc.page_content for doc in documents])\n",
        "        \n",
        "        # Pair documents with their scores\n",
        "        doc_scores = list(zip(documents, scores))\n",
        "        \n",
        "        # Sort by relevance score (descending) and return top_k\n",
        "        sorted_docs = sorted(doc_scores, key=lambda x: x[1], reverse=True)\n",
        "        return [doc for doc, _ in sorted_docs[:self._top_k]]\n",
        "    \n",
        "    def _calculate_relevance_scores(self, query: str, documents: List[str]) -> List[float]:\n",
        "        \"\"\"Calculate relevance scores for multiple documents efficiently.\"\"\"\n",
        "        # Apply prompt template to all query-document pairs\n",
        "        texts = [self._prompt_template(query, doc) for doc in documents]\n",
        "        \n",
        "        # Tokenize all texts in batch\n",
        "        batch_dict = self._tokenizer(\n",
        "            texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=self._max_length,\n",
        "        )\n",
        "        \n",
        "        # Move to device\n",
        "        batch_dict = {k: v.to(self._device) for k, v in batch_dict.items()}\n",
        "        \n",
        "        # Generate relevance scores\n",
        "        with torch.inference_mode():\n",
        "            logits = self._model(**batch_dict).logits\n",
        "            scores = logits.view(-1).cpu().tolist()\n",
        "        \n",
        "        return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Initialize Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embedding model: nvidia/llama-3.2-nv-embedqa-1b-v2\n",
            "Loading model with trust_remote_code=True for custom NVIDIA architecture...\n",
            "âœ… Embedding model loaded on cuda\n",
            "Loading reranking model: nvidia/llama-3.2-nv-rerankqa-1b-v2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/nvidia/llama-3.2-nv-rerankqa-1b-v2:\n",
            "- llama_bidirectional_model.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Reranking model loaded on cuda\n",
            "Initializing LLM: nvidia/nvidia-nemotron-nano-9b-v2\n",
            "âœ… All models initialized successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/chris/Code/NVIDIA/workshop-build-an-agent/.venv/lib/python3.13/site-packages/langchain_nvidia_ai_endpoints/_common.py:229: UserWarning: Found nvidia/nvidia-nemotron-nano-9b-v2 in available_models, but type is unknown and inference may fail.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Initialize the embedding model (Hugging Face)\n",
        "embeddings = HuggingFaceNVIDIAEmbeddings(\n",
        "    model_name=RETRIEVER_EMBEDDING_MODEL,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Initialize the reranker model (Hugging Face)\n",
        "reranker = HuggingFaceNVIDIAReranker(\n",
        "    model_name=RETRIEVER_RERANK_MODEL,\n",
        "    device=device,\n",
        "    top_k=3\n",
        ")\n",
        "\n",
        "# Initialize the LLM (NVIDIA AI Endpoints)\n",
        "print(f\"Initializing LLM: {LLM_MODEL}\")\n",
        "llm = ChatNVIDIA(model=LLM_MODEL, temperature=0.6, top_p=0.95, max_completion_tokens=8192)\n",
        "print(\"âœ… All models initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Loading and Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Reading knowledge base data from ../data/it-knowledge-base\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 3230.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 12 documents\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Check if data directory exists\n",
        "if not DATA_DIR.exists():\n",
        "    print(f\"Warning: Data directory {DATA_DIR} does not exist.\")\n",
        "    print(\"Creating sample documents for demonstration...\")\n",
        "    \n",
        "    # Create sample IT knowledge base documents\n",
        "    sample_docs = [\n",
        "        Document(\n",
        "            page_content=\"Password Policy: All employees must use passwords with at least 12 characters, including uppercase, lowercase, numbers, and special characters. Passwords must be changed every 90 days.\",\n",
        "            metadata={\"source\": \"password_policy.txt\"}\n",
        "        ),\n",
        "        Document(\n",
        "            page_content=\"VPN Access: To connect to the company VPN, use the Cisco AnyConnect client. Download it from the IT portal and use your employee credentials. Contact IT support if you experience connection issues.\",\n",
        "            metadata={\"source\": \"vpn_guide.txt\"}\n",
        "        ),\n",
        "        Document(\n",
        "            page_content=\"Email Setup: Configure your email client with IMAP settings: Server: mail.company.com, Port: 993, Security: SSL/TLS. For SMTP: Server: smtp.company.com, Port: 587, Security: STARTTLS.\",\n",
        "            metadata={\"source\": \"email_setup.txt\"}\n",
        "        ),\n",
        "        Document(\n",
        "            page_content=\"Software Installation: All software installations require IT approval. Submit requests through the IT portal. Unauthorized software installation may result in security violations.\",\n",
        "            metadata={\"source\": \"software_policy.txt\"}\n",
        "        ),\n",
        "        Document(\n",
        "            page_content=\"Backup Policy: All critical data must be backed up daily. Use the company-approved backup solution. Personal devices are not covered under the backup policy.\",\n",
        "            metadata={\"source\": \"backup_policy.txt\"}\n",
        "        )\n",
        "    ]\n",
        "    docs = sample_docs\n",
        "else:\n",
        "    # Load documents from directory\n",
        "    _LOGGER.info(f\"Reading knowledge base data from {DATA_DIR}\")\n",
        "    data_loader = DirectoryLoader(\n",
        "        DATA_DIR,\n",
        "        glob=\"**/*\",\n",
        "        loader_cls=TextLoader,\n",
        "        show_progress=True,\n",
        "    )\n",
        "    docs = data_loader.load()\n",
        "\n",
        "print(f\"Loaded {len(docs)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Create Vector Database and Retriever\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Splitting documents into chunks\n",
            "INFO:__main__:Creating FAISS vector database with 155 chunks\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 155 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:faiss.loader:Loading faiss with AVX512 support.\n",
            "INFO:faiss.loader:Successfully loaded faiss with AVX512 support.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector database created successfully!\n",
            "Retriever with reranking created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Split the data into chunks\n",
        "_LOGGER.info(f\"Splitting documents into chunks\")\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE, \n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(f\"Created {len(chunks)} chunks\")\n",
        "\n",
        "# Create FAISS vector database\n",
        "_LOGGER.info(f\"Creating FAISS vector database with {len(chunks)} chunks\")\n",
        "vectordb = FAISS.from_documents(chunks, embeddings)\n",
        "print(\"Vector database created successfully!\")\n",
        "\n",
        "# Create base retriever\n",
        "kb_retriever = vectordb.as_retriever(\n",
        "    search_type=\"similarity\", \n",
        "    search_kwargs={\"k\": 6}\n",
        ")\n",
        "\n",
        "# Combine retriever with reranker\n",
        "retriever = ContextualCompressionRetriever(\n",
        "    base_retriever=kb_retriever,\n",
        "    base_compressor=reranker,\n",
        ")\n",
        "\n",
        "print(\"Retriever with reranking created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create LangGraph Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… LangGraph ReAct agent created successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/chris/Code/NVIDIA/workshop-build-an-agent/.venv/lib/python3.13/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:715: UserWarning: Model 'nvidia/nvidia-nemotron-nano-9b-v2' is not known to support tools. Your tool binding may fail at inference time.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Create the retriever tool for agentic use\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever=retriever,\n",
        "    name=\"company_llc_it_knowledge_base\",\n",
        "    description=(\n",
        "        \"Search the internal IT knowledge base for Company LLC IT related questions and policies. \"\n",
        "        \"Use this tool to find information about passwords, VPN, email setup, software policies, and other IT topics.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Define the system prompt for the agent\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are an IT help desk support agent.\\n\"\n",
        "    \"- Use the 'company_llc_it_knowledge_base' tool for questions likely covered by the internal IT knowledge base.\\n\"\n",
        "    \"- Always write grounded answers. If unsure, say you don't know.\\n\"\n",
        "    \"- If the knowledge base doesn't contain sufficient information, clearly state what information is missing.\\n\"\n",
        "    \"- Keep answers brief, to the point, and conversational.\"\n",
        ")\n",
        "\n",
        "# Create the ReAct agent\n",
        "agent = create_react_agent(\n",
        "    model=llm,\n",
        "    tools=[retriever_tool],\n",
        "    prompt=SYSTEM_PROMPT,\n",
        ")\n",
        "\n",
        "print(\"âœ… LangGraph ReAct agent created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test the Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Query: What are the password requirements for employees?\n",
            "============================================================\n",
            "Agent Response:\n",
            "The password requirements for employees are as follows:  \n",
            "- Minimum of **12 characters**.  \n",
            "- Must include at least one uppercase letter, one lowercase letter, one number, and one special character.  \n",
            "- Avoid common words, sequential numbers, or previously used passwords.  \n",
            "\n",
            "Let me know if you need further clarification!\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test the agent with sample queries\n",
        "test_queries = [\n",
        "    \"What are the password requirements for employees?\",\n",
        "    \"How do I set up VPN access?\",\n",
        "    \"What are the email server settings?\",\n",
        "    \"Can I install my own software?\",\n",
        "    \"What's the backup policy?\"\n",
        "]\n",
        "\n",
        "def test_agent(query: str):\n",
        "    \"\"\"Test the agent with a single query.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    try:\n",
        "        # Invoke the agent\n",
        "        response = agent.invoke({\"messages\": [(\"user\", query)]})\n",
        "        \n",
        "        # Extract the final message\n",
        "        final_message = response[\"messages\"][-1].content\n",
        "        print(f\"Agent Response:\\n{final_message}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "    \n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Test with the first query\n",
        "test_agent(test_queries[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates a **hybrid approach** combining:\n",
        "\n",
        "### ðŸ  Local Hugging Face Models:\n",
        "- **Embedding**: `nvidia/llama-3.2-nv-embedqa-1b-v2`\n",
        "- **Reranking**: `nvidia/llama-3.2-nv-rerankqa-1b-v2`\n",
        "\n",
        "### â˜ï¸ NVIDIA AI Endpoints:\n",
        "- **LLM**: `nvidia/nvidia-nemotron-nano-9b-v2`\n",
        "\n",
        "### Key Benefits:\n",
        "\n",
        "1. **Best of Both Worlds**:\n",
        "   - Local models for embedding/reranking (privacy, performance, cost)\n",
        "   - Managed LLM service (latest models, no infrastructure)\n",
        "\n",
        "2. **Performance Optimizations**:\n",
        "   - GPU acceleration for local models\n",
        "   - Efficient batching and memory management\n",
        "   - Proper query/passage prefixing as per model specs\n",
        "\n",
        "3. **Production Ready**:\n",
        "   - Complete LangGraph agent with tool integration\n",
        "   - Proper error handling and logging\n",
        "   - Interactive testing capabilities\n",
        "\n",
        "### Architecture Flow:\n",
        "\n",
        "1. **Document Processing**: Text splitting and chunking\n",
        "2. **Embedding**: Local Hugging Face model generates embeddings\n",
        "3. **Vector Search**: FAISS similarity search (k=6)\n",
        "4. **Reranking**: Local Hugging Face model reranks to top 3\n",
        "5. **Generation**: NVIDIA AI Endpoints LLM generates final response\n",
        "\n",
        "### Usage:\n",
        "\n",
        "- Run cells sequentially to set up the complete system\n",
        "- Use `test_agent()` function for individual queries\n",
        "- Use `chat_with_agent()` for interactive conversations\n",
        "- Type 'test' in chat to run all predefined test queries\n",
        "\n",
        "### Requirements:\n",
        "\n",
        "- NVIDIA API key for LLM access\n",
        "- GPU recommended for local models (falls back to CPU)\n",
        "- Internet connection for model downloads and LLM API calls\n",
        "\n",
        "The models used are:\n",
        "- [nvidia/llama-3.2-nv-embedqa-1b-v2](https://huggingface.co/nvidia/llama-3.2-nv-embedqa-1b-v2)\n",
        "- [nvidia/llama-3.2-nv-rerankqa-1b-v2](https://huggingface.co/nvidia/llama-3.2-nv-rerankqa-1b-v2)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
